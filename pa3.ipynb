{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "\n",
    "allfiles_tokens = [] # a list of doc of tokens\n",
    "vocabulary = dict() # all unrepaeat tokens\n",
    "sorted_vocabulary = [] \n",
    "\n",
    "\n",
    "# tokenize\n",
    "\n",
    "path = './IRTM/'\n",
    "filenames = os.listdir(path)\n",
    "for file in filenames:\n",
    "    fp = open(f'{path}{file}', 'r')    \n",
    "    singlefile_tokens = []\n",
    "    for line in fp:\n",
    "        modified_line = []\n",
    "        for i in line:\n",
    "            if i.isalpha() or i == ' ':\n",
    "                modified_line.append(i)\n",
    "        modified_line = ''.join(modified_line)\n",
    "\n",
    "        singleline_tokens = modified_line.split(' ')\n",
    "        for token in singleline_tokens:\n",
    "            if token:\n",
    "                singlefile_tokens.append(token)\n",
    "    fp.close()\n",
    "\n",
    "    for i in range(len(singlefile_tokens)):\n",
    "        singlefile_tokens[i] = singlefile_tokens[i].lower()\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(singlefile_tokens)):\n",
    "        singlefile_tokens[i] = ps.stem(singlefile_tokens[i])\n",
    "\n",
    "    singlefile_results = []\n",
    "    nltk_stopwords = ['news', 'im', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    nltk_stopwords = set(nltk_stopwords)\n",
    "    for i in range(len(singlefile_tokens)):\n",
    "        if singlefile_tokens[i] not in nltk_stopwords:\n",
    "            singlefile_results.append(singlefile_tokens[i])\n",
    "    \n",
    "    allfiles_tokens.append(singlefile_results)\n",
    "\n",
    "    \n",
    "# save in dictionary\n",
    "\n",
    "allfiles_tokens_set = [None] * len(allfiles_tokens) # a list of doc of unrepeat tokens\n",
    "\n",
    "for i, document in enumerate(allfiles_tokens):\n",
    "    allfiles_tokens_set[i] = set(document)\n",
    "    for token in allfiles_tokens_set[i]:\n",
    "        if token in vocabulary:\n",
    "            vocabulary[token] += 1\n",
    "        else:\n",
    "            vocabulary[token] = 1\n",
    "\n",
    "\n",
    "\n",
    "# sort the dictionary\n",
    "\n",
    "for token in vocabulary:\n",
    "    sorted_vocabulary.append(token)\n",
    "sorted_vocabulary.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_class = 13\n",
    "training_set = [None] * number_of_class\n",
    "count_docs = 0\n",
    "\n",
    "fp = open('training.txt', 'r')\n",
    "for line in fp:\n",
    "    words = line.strip().split(' ')\n",
    "    class_name = int(words[0])\n",
    "    training_set[class_name-1] = []\n",
    "    for i in range(1, len(words)):\n",
    "        training_set[class_name-1].append(words[i])\n",
    "        count_docs += 1\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "prior_each_class = [0] * number_of_class\n",
    "count_docs_each_class = [0] * number_of_class\n",
    "tokens_each_class = [None] * number_of_class\n",
    "count_tokens_each_class = [0] * number_of_class\n",
    "count_token_freq = dict()\n",
    "selected_token = []\n",
    "\n",
    "# count prior\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    for document in training_set[i]:\n",
    "        count_docs_each_class[i] += 1\n",
    "for i in range(number_of_class):\n",
    "    prior_each_class[i] = count_docs_each_class[i] / count_docs\n",
    "\n",
    "    \n",
    "# count all tokens in each class\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    tokens_each_class[i] = []\n",
    "    for document in training_set[i]:\n",
    "        for token in allfiles_tokens[int(document)-1]:\n",
    "            tokens_each_class[i].append(token)\n",
    "            \n",
    "\n",
    "# count numbers of all tokens in each class\n",
    "            \n",
    "for i in range(number_of_class):\n",
    "    count_tokens_each_class[i] = len(tokens_each_class[i])\n",
    "\n",
    "\n",
    "# count each token frequency in each class\n",
    "    \n",
    "for word in vocabulary:\n",
    "    count_token_freq[word] = []\n",
    "    for class_name in range(number_of_class):\n",
    "        count_token_freq[word].append(Counter(tokens_each_class[class_name])[word])\n",
    "\n",
    "# count Likelihood Ratios in each class\n",
    "        \n",
    "for class_c in range(number_of_class):\n",
    "    likelihood_tokens = dict()\n",
    "    top_k_tokens = []\n",
    "    for token in tokens_each_class[class_c]:\n",
    "        n_11 = count_token_freq[token][class_c] # present and on topic\n",
    "        n_01 = sum(count_token_freq[token]) - n_11  # present and off topic\n",
    "        n_10 = count_tokens_each_class[class_c] - n_11 # absent and on topic\n",
    "        n_00 = sum(count_tokens_each_class) - sum(count_token_freq[token]) - n_10 # absent and off topic\n",
    "        N = sum(count_tokens_each_class)\n",
    "        \n",
    "        a = (((n_11+n_01)/N)**n_11) * ((1-(n_11+n_01)/N)**n_10) * (((n_11+n_01)/N)**n_01) * ((1-(n_11+n_01)/N)**n_00)\n",
    "        b = ((n_11/(n_11+n_10))**n_11) * ((1-n_11/(n_11+n_10))**n_10) * ((n_01/(n_01+n_00))**n_01) * ((1-n_01/(n_01+n_00))**n_00)\n",
    "        if a!=0 and b != 0:\n",
    "            x = -2 * math.log(a/b)\n",
    "        else:\n",
    "            x = 0\n",
    "        likelihood_tokens[token] = x\n",
    "        \n",
    "    y = heapq.nlargest(38, likelihood_tokens.items(), key=itemgetter(1))\n",
    "    top_k_tokens = list(map(itemgetter(0), y))\n",
    "    \n",
    "    for token in top_k_tokens:\n",
    "        selected_token.append(token)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_vocabulary = set()\n",
    "selected_tokens_each_class = [None] * number_of_class\n",
    "count_selected_tokens_each_class = [None] * number_of_class\n",
    "count_selected_token_freq = dict()\n",
    "selected_vocabulary = set(selected_token)\n",
    "condProb = dict()\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    selected_tokens_each_class[i] = []\n",
    "    for token in tokens_each_class[i]:\n",
    "        if token in selected_vocabulary:\n",
    "            selected_tokens_each_class[i].append(token)\n",
    "    count_selected_tokens_each_class[i] = len(selected_tokens_each_class[i])\n",
    "    \n",
    "for token in selected_vocabulary:\n",
    "    count_selected_token_freq[token] = []\n",
    "    condProb[token] = []\n",
    "    for i in range(number_of_class):\n",
    "        count_selected_token_freq[token].append(Counter(selected_tokens_each_class[i])[token]) \n",
    "    for i in range(number_of_class):\n",
    "        a = count_selected_token_freq[token][i] + 1\n",
    "        b = count_selected_tokens_each_class[i] + len(selected_vocabulary)\n",
    "        condProb[token].append(a/b)\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training = []\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    for document in training_set[i]:\n",
    "        total_training.append(int(document))\n",
    "        \n",
    "total_training = sorted(total_training)\n",
    "\n",
    "savefilename = 'hw3_1.csv'  \n",
    "rp = open(savefilename, 'w')\n",
    "rp.write('Id' + ',' + 'Value' + '\\n')\n",
    "\n",
    "for document in range(1, len(allfiles_tokens)+1):\n",
    "    if document not in total_training:\n",
    "        test_tokens = allfiles_tokens[document-1]\n",
    "        score = [0] * number_of_class\n",
    "        for i in range(number_of_class):\n",
    "            for token in test_tokens:\n",
    "                if token in selected_vocabulary:\n",
    "                    score[i] += math.log(condProb[token][i])\n",
    "        class_result = int(score.index(max(score))) + 1\n",
    "        rp.write(f'{document},{class_result}\\n')\n",
    "        \n",
    "rp.close()\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
